{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cityscapes: UNET Edge Detection & Faster R-CNN Object Detection\n",
        "### Trains and evaluates UNET (boundary detection) and Faster R-CNN (object detection)\n",
        "### on the preprocessed Cityscapes dataset, optimized for memory constraints."
      ],
      "metadata": {
        "id": "CJy_RLiBfIPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Setup and Imports\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Optional: Set for CUDA debugging\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "47YdTl6YfaD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Data Preparation\n",
        "\n",
        "\n",
        "# Mount Google Drive (if running in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # !! Adjust this path to your zip file in Google Drive !!\n",
        "    zip_path = '/content/drive/MyDrive/Colab Notebooks/Sem_2_lab/preprocessed_cityscapes.zip'\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Not running in Colab or Drive not available. Assuming data is local.\")\n",
        "    # !! Set this path if running locally !!\n",
        "    zip_path = 'preprocessed_cityscapes.zip'\n",
        "\n",
        "extract_path = '/content/preprocessed_cityscapes' if 'google.colab' in str(get_ipython()) else 'preprocessed_cityscapes'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the dataset\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(f\"Error: Zip file not found at {zip_path}. Please check the path.\")\n",
        "\n",
        "# Define data directories\n",
        "image_dir = os.path.join(extract_path, 'images')\n",
        "boundary_mask_dir = os.path.join(extract_path, 'boundaries')\n",
        "full_mask_dir = os.path.join(extract_path, 'masks') # For Faster R-CNN\n",
        "\n",
        "# Verify directories\n",
        "if not os.path.isdir(image_dir) or not os.path.isdir(boundary_mask_dir) or not os.path.isdir(full_mask_dir):\n",
        "    print(\"Error: Data directories not found after extraction.\")\n",
        "else:\n",
        "    print(\"Data directories verified.\")\n",
        "    print(f\"Found {len(os.listdir(image_dir))} images.\")\n",
        "    print(f\"Found {len(os.listdir(boundary_mask_dir))} boundary masks.\")\n",
        "    print(f\"Found {len(os.listdir(full_mask_dir))} full masks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-c20rlpfxAh",
        "outputId": "36b6e971-90eb-4ab5-c19d-6cf4b7c8a6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extracting /content/drive/MyDrive/Colab Notebooks/Sem_2_lab/preprocessed_cityscapes.zip to /content/preprocessed_cityscapes...\n",
            "Extraction complete.\n",
            "Data directories verified.\n",
            "Found 2975 images.\n",
            "Found 2975 boundary masks.\n",
            "Found 2975 full masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNET for Boundary / Edge Detection"
      ],
      "metadata": {
        "id": "6i_gwj7NgTIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNET Model Definition ---\n",
        "class UNetBoundary(nn.Module):\n",
        "    \"\"\" UNET architecture for boundary detection (outputs logits). \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNetBoundary, self).__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c):\n",
        "            # Standard UNET conv block with BatchNorm\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        # Encoder path\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.enc4 = conv_block(256, 512) # Bottleneck\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        self.dec3 = conv_block(512 + 256, 256)\n",
        "        self.dec2 = conv_block(256 + 128, 128)\n",
        "        self.dec1 = conv_block(128 + 64, 64)\n",
        "\n",
        "        # Final 1x1 convolution for output channel\n",
        "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        e4 = self.enc4(self.pool(e3))\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d3 = self.up(e4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up(d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        out = self.final(d1) # Output logits\n",
        "        return out"
      ],
      "metadata": {
        "id": "22pGh4T8gihc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNET Dataset ---\n",
        "class CityscapesBoundaryDataset(Dataset):\n",
        "    \"\"\" Loads Cityscapes images and boundary masks. \"\"\"\n",
        "    def __init__(self, image_paths, boundary_paths, transform=None, mask_transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.boundary_paths = boundary_paths\n",
        "        self.transform = transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "            mask = Image.open(self.boundary_paths[idx]).convert('L')\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            if self.mask_transform:\n",
        "                 mask = self.mask_transform(mask)\n",
        "            else:\n",
        "                 mask = transforms.ToTensor()(mask) # Default transform\n",
        "\n",
        "            mask = mask.float() # Ensure mask is float\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data index {idx} ({self.image_paths[idx]}): {e}\")\n",
        "            return None, None # Return None on error, handled by collate_fn\n",
        "\n",
        "        return img, mask"
      ],
      "metadata": {
        "id": "8f3y1fGDgpm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNET Training Function (Chunked) ---\n",
        "def train_unet_chunk(model, image_paths, boundary_paths, device, model_path, num_epochs=3, batch_size=8, accumulation_steps=4, learning_rate=1e-4):\n",
        "    \"\"\" Trains UNET on a data chunk with mixed precision and gradient accumulation. \"\"\"\n",
        "    img_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    mask_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    dataset = CityscapesBoundaryDataset(image_paths, boundary_paths, transform=img_transform, mask_transform=mask_transform)\n",
        "\n",
        "    # Collate function to filter out None items from dataset errors\n",
        "    def collate_fn_filter_none(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return None, None\n",
        "        return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn_filter_none)\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.BCEWithLogitsLoss() # Use with logits output\n",
        "    scaler = GradScaler(enabled=(device.type == 'cuda')) # For mixed precision\n",
        "\n",
        "    print(f\"Starting training on {len(dataset)} images for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        processed_batches = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100, unit='batch')\n",
        "        for step, batch_data in enumerate(pbar):\n",
        "            if batch_data[0] is None: continue # Skip bad batches\n",
        "\n",
        "            images, masks = batch_data\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast(enabled=(device.type == 'cuda')): # Mixed precision context\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                if torch.isnan(loss):\n",
        "                    print(f\"NaN loss detected at step {step}! Skipping batch.\")\n",
        "                    continue\n",
        "                loss = loss / accumulation_steps # Scale loss for accumulation\n",
        "\n",
        "            scaler.scale(loss).backward() # Scale loss, calculate gradients\n",
        "\n",
        "            # Optimizer step after accumulating gradients\n",
        "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(loader):\n",
        "                # Optional: Gradient Clipping\n",
        "                # scaler.unscale_(optimizer)\n",
        "                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss.item() * accumulation_steps\n",
        "            total_loss += batch_loss\n",
        "            processed_batches += 1\n",
        "            pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
        "\n",
        "            del images, masks, outputs, loss, batch_data\n",
        "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "        if processed_batches > 0:\n",
        "             avg_loss = total_loss / processed_batches\n",
        "             print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "        else:\n",
        "             print(f\"Epoch {epoch+1} had no processed batches.\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Checkpoint saved to {model_path}\")"
      ],
      "metadata": {
        "id": "Qv-Mt4cIg4RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNET Evaluation Metrics (IoU, Dice) ---\n",
        "def compute_iou_and_dice(preds, targets, threshold=0.5, smooth=1e-6):\n",
        "    \"\"\" Computes IoU and Dice Score for binary segmentation. \"\"\"\n",
        "    preds = torch.sigmoid(preds) # Convert logits to probabilities\n",
        "    preds_bin = (preds > threshold).float()\n",
        "    targets_bin = (targets > 0.5).float()\n",
        "\n",
        "    if preds_bin.ndim == 4: preds_bin = preds_bin.squeeze(1)\n",
        "    if targets_bin.ndim == 4: targets_bin = targets_bin.squeeze(1)\n",
        "\n",
        "    intersection = (preds_bin * targets_bin).sum(dim=(1, 2))\n",
        "    pred_sum = preds_bin.sum(dim=(1, 2))\n",
        "    target_sum = targets_bin.sum(dim=(1, 2))\n",
        "    union = pred_sum + target_sum\n",
        "\n",
        "    iou = (intersection + smooth) / (union - intersection + smooth)\n",
        "    dice = (2 * intersection + smooth) / (union + smooth)\n",
        "    return iou.mean().item(), dice.mean().item()\n",
        "\n",
        "def evaluate_unet(model, dataloader, device, threshold=0.5):\n",
        "    \"\"\" Evaluates the UNET model using IoU and Dice score. \"\"\"\n",
        "    model.eval()\n",
        "    total_iou, total_dice = 0.0, 0.0\n",
        "    num_batches = 0\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Evaluating UNET\", ncols=100, unit='batch')\n",
        "        for batch_data in pbar:\n",
        "            if batch_data[0] is None: continue\n",
        "            images, masks = batch_data\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            masks = masks.to(device, non_blocking=True) # Keep masks on GPU for consistency if possible\n",
        "\n",
        "            outputs = model(images)\n",
        "            # Compute metrics on CPU to avoid potential GPU memory issues during aggregation\n",
        "            iou, dice = compute_iou_and_dice(outputs.cpu(), masks.cpu(), threshold=threshold)\n",
        "\n",
        "            total_iou += iou\n",
        "            total_dice += dice\n",
        "            num_batches += 1\n",
        "            pbar.set_postfix(iou=f\"{iou:.4f}\", dice=f\"{dice:.4f}\")\n",
        "\n",
        "            del images, masks, outputs, batch_data\n",
        "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "    avg_iou = total_iou / num_batches if num_batches > 0 else 0\n",
        "    avg_dice = total_dice / num_batches if num_batches > 0 else 0\n",
        "    print(f\"\\nEvaluation Results - Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f}\")\n",
        "    return avg_iou, avg_dice"
      ],
      "metadata": {
        "id": "lgaRyFvKg4Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNET Visualization ---\n",
        "def visualize_unet_predictions(model, dataset, device, num_images=5, threshold=0.5):\n",
        "    \"\"\" Visualizes UNET predictions against ground truth. \"\"\"\n",
        "    model.eval()\n",
        "    num_images = min(num_images, len(dataset))\n",
        "    if num_images == 0: return\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(12, num_images * 4))\n",
        "    if num_images == 1: axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    # Inverse normalization transform for display\n",
        "    inv_normalize = transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std=[1/0.229, 1/0.224, 1/0.225]\n",
        "    )\n",
        "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
        "    print(f\"Visualizing {num_images} UNET predictions...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            img, mask = dataset[idx]\n",
        "            if img is None: continue\n",
        "            img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(img_tensor)\n",
        "            pred_prob = torch.sigmoid(output)\n",
        "            pred_mask = (pred_prob > threshold).float().cpu().squeeze(0)\n",
        "\n",
        "            # Prepare images for display\n",
        "            img_display = TF.to_pil_image(inv_normalize(img).cpu())\n",
        "            mask_display = TF.to_pil_image(mask.cpu())\n",
        "            pred_display = TF.to_pil_image(pred_mask)\n",
        "\n",
        "            # Plot\n",
        "            axes[i, 0].imshow(img_display); axes[i, 0].set_title(\"Input Image\"); axes[i, 0].axis('off')\n",
        "            axes[i, 1].imshow(mask_display, cmap='gray'); axes[i, 1].set_title(\"Ground Truth\"); axes[i, 1].axis('off')\n",
        "            axes[i, 2].imshow(pred_display, cmap='gray'); axes[i, 2].set_title(\"Prediction\"); axes[i, 2].axis('off')\n",
        "\n",
        "            del img_tensor, output, pred_prob, pred_mask\n",
        "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vzj04wyAhJcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNET Training Execution"
      ],
      "metadata": {
        "id": "peKFOrDQhNyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNET Main Training Loop ---\n",
        "def run_unet_training():\n",
        "    print(\"--- Starting UNET Training ---\")\n",
        "    # --- Configuration ---\n",
        "    unet_model_path = 'unet_boundary_checkpoint.pth'\n",
        "    unet_num_epochs_per_chunk = 40\n",
        "    unet_batch_size = 8         # Adjust based on GPU VRAM\n",
        "    unet_accumulation_steps = 4 # Effective batch size = batch_size * accumulation_steps\n",
        "    unet_learning_rate = 1e-3\n",
        "    unet_chunk_size = 1500      # Images per training chunk\n",
        "    unet_eval_chunk_size = 200  # Images for evaluation subset\n",
        "    unet_vis_count = 5\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- Data Paths ---\n",
        "    all_image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
        "    all_boundary_paths = sorted(glob.glob(os.path.join(boundary_mask_dir, '*.png')))\n",
        "    if len(all_image_paths) != len(all_boundary_paths) or not all_image_paths:\n",
        "        print(\"Error: Image/mask mismatch or no data found.\")\n",
        "        return\n",
        "    print(f\"Total image/mask pairs found: {len(all_image_paths)}\")\n",
        "\n",
        "    # --- Model Initialization ---\n",
        "    model = UNetBoundary(in_channels=3, out_channels=1)\n",
        "    if os.path.exists(unet_model_path):\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(unet_model_path, map_location=device))\n",
        "            print(f\"Loaded UNET checkpoint: {unet_model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load checkpoint: {e}. Starting fresh.\")\n",
        "    model.to(device)\n",
        "\n",
        "    # --- Training Loop (Chunked) ---\n",
        "    num_chunks = (len(all_image_paths) + unet_chunk_size - 1) // unet_chunk_size\n",
        "    print(f\"Starting training in {num_chunks} chunks...\")\n",
        "    for i in range(0, len(all_image_paths), unet_chunk_size):\n",
        "        chunk_start, chunk_end = i, min(i + unet_chunk_size, len(all_image_paths))\n",
        "        chunk_images = all_image_paths[chunk_start:chunk_end]\n",
        "        chunk_masks = all_boundary_paths[chunk_start:chunk_end]\n",
        "        print(f\"\\n--- Training UNET Chunk {i//unet_chunk_size + 1}/{num_chunks} ({len(chunk_images)} images) ---\")\n",
        "        train_unet_chunk(model, chunk_images, chunk_masks, device, unet_model_path,\n",
        "                         num_epochs=unet_num_epochs_per_chunk, batch_size=unet_batch_size,\n",
        "                         accumulation_steps=unet_accumulation_steps, learning_rate=unet_learning_rate)\n",
        "    print(\"\\n--- UNET Training Finished ---\")\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    print(\"\\n--- Evaluating UNET Model ---\")\n",
        "    eval_image_paths = all_image_paths[:unet_eval_chunk_size]\n",
        "    eval_mask_paths = all_boundary_paths[:unet_eval_chunk_size]\n",
        "    eval_img_transform = transforms.Compose([\n",
        "        transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    eval_mask_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    eval_dataset = CityscapesBoundaryDataset(eval_image_paths, eval_mask_paths, transform=eval_img_transform, mask_transform=eval_mask_transform)\n",
        "\n",
        "    def collate_fn_filter_none(batch): # Reusable collate fn\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return None, None\n",
        "        return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=unet_batch_size * 2, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn_filter_none)\n",
        "    evaluate_unet(model, eval_loader, device)\n",
        "\n",
        "    # --- Visualization ---\n",
        "    print(\"\\n--- Visualizing UNET Predictions ---\")\n",
        "    # Dataset for visualization (no normalization on image)\n",
        "    vis_img_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    vis_mask_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    # Dataset for model input during visualization (needs normalization)\n",
        "    vis_eval_dataset = CityscapesBoundaryDataset(all_image_paths, all_boundary_paths, transform=eval_img_transform, mask_transform=eval_mask_transform)\n",
        "\n",
        "    if os.path.exists(unet_model_path): # Ensure latest model is loaded\n",
        "        model.load_state_dict(torch.load(unet_model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    visualize_unet_predictions(model, vis_eval_dataset, device, num_images=unet_vis_count)"
      ],
      "metadata": {
        "id": "weJYaelhhcos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run UNET pipeline\n",
        "run_unet_training()"
      ],
      "metadata": {
        "id": "WXk28ETehfcB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}